{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9jzseT8WDHRDQVy8EGxlo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steph-crypt/project-4/blob/main/Prompt_engineering_small_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flan-T5 LLM Prompt Engineering"
      ],
      "metadata": {
        "id": "XrL-xlJDkMG5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKtna1P18InZ",
        "outputId": "b6899da1-b151-4f37-a15f-df083ea846f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# T5-small LLM Prompt Engineering\n"
      ],
      "metadata": {
        "id": "WsrdIWPM4hO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Define a function to summarize a chunk of text\n",
        "def summarize_text(text, max_input_length=512, max_output_length=150):\n",
        "    input_text = f\"summarize: {text.strip()}\"\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=max_output_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "df = pd.read_csv(\"/top3_products.csv\")\n",
        "df[\"prompt\"] = df[\"positive_reviews\"].fillna(\"\") + \"\\n\" + df[\"negative_reviews\"].fillna(\"\")\n",
        "\n",
        "grouped = df.groupby(\"name\")\n",
        "\n",
        "# Loop through each category\n",
        "for category, group in grouped:\n",
        "    reviews = \" \".join(group[\"prompt\"].astype(str).tolist())\n",
        "    summary = summarize_text(reviews)\n",
        "\n",
        "    print(f\"\\n=== Summary for Category: {category} ===\\n\")\n",
        "    print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxVPuFMO4ift",
        "outputId": "ba37379c-bf3a-4d1e-b752-54cedd0cca30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Summary for Category: All-New Fire HD 8 Kids Edition Tablet, 8 HD Display, 32 GB, Blue Kid-Proof Case ===\n",
            "\n",
            "amazon kid allowing unlimited access almost childrenäôs show apps normally paid amazon-fire hd 8 '' 32gb tablet granddaughter 6th birthday quickly outgrown tablet although age appropriate tablet enough storage space especially add micro-sd card allow downloading apps operating system also permit downloading earlier tablet permit extremely happy amazon fire tablet evidenced amount time spends using awesome kid tablet 100 apple person always hating kindles due limited appstore found get google play store device fell love kid version 100 worth extra money isnäôt case make money\n",
            "\n",
            "=== Summary for Category: All-New Fire HD 8 Kids Edition Tablet, 8 HD Display, 32 GB, Pink Kid-Proof Case ===\n",
            "\n",
            "easy use bought 2 year old since kept fighting sister ipads great alternative cost much less 12 month freetime great prime member get big discount continue service 12 free month build quality great free child bumper work great well also come free 2 year warranty amazon piece mind case kid break hard included bumper great product great device child like closed ecosystem kidn't seen't like daughter made purchase mistake day 2 turnoff one click buying alson't like lack google apps device great screen weight bumper tough easy clean overall would recommend 4yo wonderful tablet\n",
            "\n",
            "=== Summary for Category: All-New Fire HD 8 Tablet with Alexa, 8 HD Display, 16 GB, Marine Blue - with Special Offers ===\n",
            "\n",
            "a+ using couple 'cheaper generic android tablet priced 50-100 say fire tablet beat without question aspect.. faster better display etc etc... prime member course geared towards prime/etc however easy enuf side load play store get apps need generic tab getting tosed've already purchased 2nd fire tab amazing little tablet purchased amazon fire hd 8 8 '' tablet 16gb 7th generation 2017 release punch red little 2 month ago use traveling home project state found use plane watch movie use check email surf\n",
            "\n",
            "=== Summary for Category: All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Black ===\n",
            "\n",
            "ipad air 2 128gb bf 375 playing around hd fire returned air 2 met need much lower price added 64gb sd card downloading movie netflix hd display clear battery life could little better overall'm satisfied free stuff get amazon prime account icing cake dig amazing tablet price wanted tablet decent screen size could play game could stream movie netflix prime could check e-mail could skype etc remaining affordable 100 yet see tablet price range thing effortlessly remaining affordable tablet handled whatever throw h\n",
            "\n",
            "=== Summary for Category: Amazon - Echo Plus w/ Built-In Hub - Silver ===\n",
            "\n",
            "alexa rock present husband originally asked radio home office set relatively quick amazed sound quality loved versatility could specific radio station played random song well tell joke etc fun u looking getting smaller  dot '' room commented would never get rid's fun functional smart bulb handy well love amazon echo plus two dot plus four fire stick hub philip hue lamp family christmas 2017. want husband use alexa stay organized business date reminder way go alexa review yesterday evening decided purchase amazon echo machine intrusive yeah guess could allow purchased home\n",
            "\n",
            "=== Summary for Category: Amazon Echo Show Alexa-enabled Bluetooth Speaker with 7\" Screen ===\n",
            "\n",
            "amazon app holder highly recommended simple powerful little hesitant spend money get glad alexa always answer say command noticed probably 95 right command given love fact see thing screen news briefing picture/video's nice also love video call option planning getting one grandma technologically advanced could sit table talk grandkids worth every penny terrific product jealous son showed u amazon show thing capable since aol eliminated instant messaging wife less able communicate office separate floor got two show two spot intercom easily use alexa music many thing probably start using smart\n",
            "\n",
            "=== Summary for Category: Amazon Kindle Paperwhite - eBook reader - 4 GB - 6 monochrome Paperwhite - touchscreen - Wi-Fi - black,,, ===\n",
            "\n",
            "daughter's 1st kindle several kindle version daughter bought birthday love reading several seriesn't carry bag full bulky book trip use light read night store many book small space want reread book simply go account download's kindle battery last long time take school read time took mine 2 week trip galapagos used airport flight several times every day since internet tv ship took little space weight restricted baggage limitn't recharge also's much easier hold v big book great backlight isnt\n",
            "\n",
            "=== Summary for Category: Echo (White),,,\n",
            "Echo (White),,, ===\n",
            "\n",
            "love echo want device performs command alexa name.she's constant companion kitchen sings tell story share recipe tell story quote bible verse give weather forecast much more.if add additional device control light thermostat grandson really like alexa tell stories.this inanimate object begin feel like real person longer around love... amazon echo become critical appliance kitchen admittedly bought echo serve yet another cool toy around.\n",
            "\n",
            "=== Summary for Category: Fire HD 8 Tablet with Alexa, 8 HD Display, 16 GB, Tangerine - with Special Offers ===\n",
            "\n",
            "exceptional tablet amazon fire hd 8 awesome device price tablet mind boggling tablet everything would expect want tablet amazon store amazon prime preloaded's much fun easy use performance tablet unbelievable never stop thinking modest price paid fantastic tablet feel comfortable hand design nice size adequate tablet use expand memory storage device additional 240gb believe outstanding great buy nice screen resolution bought super sale e-reader use browse web lieu laptop traveling found great value price process rapidly web amazon compare google unable download google play app although g mail work like champ camera produce decent image would\n",
            "\n",
            "=== Summary for Category: Fire Kids Edition Tablet, 7 Display, Wi-Fi, 16 GB, Pink Kid-Proof Case ===\n",
            "\n",
            "great little computer keep child entertained amazon fire kid edition tablet provides plenty storage movie autism speech delay fire kid editionn't know well would full blown tablet shocked well navigates get stuck time time key word pull apps close pause play movie etc vocal skill improved saying identifying letters- tablet'm impressed well hold attention letter understanding's 3.5 transferred screen physical material use plus's great motivation tool love bad plug tablet great like warranty comen't like charging port wire come device short fully snap device let charge device unless place carefully\n",
            "\n",
            "=== Summary for Category: Kindle Voyage E-reader, 6 High-Resolution Display (300 ppi) with Adaptive Built-in Light, PagePress Sensors, Free 3G + Wi-Fi - Includes Special Offers ===\n",
            "\n",
            "read noticeable improvement backlight exceptional direct sun readability affected polarized sunglass... great battery life... expect last long time... upgraded lost last kindle train excellent function really like smaller size reader left handed ease use haptic page turn button easy usen't even use touch screen page advance use kindle every night enjoy much would highly recommend want upgrade voyage visit local best buy hold hand feel see difference read got ta read novel biography anything need free 3g cellular coverage.\n",
            "\n",
            "=== Summary for Category: Kindle Voyage E-reader, 6 High-Resolution Display (300 ppi) with Adaptive Built-in Light, PagePress Sensors, Wi-Fi - Includes Special Offers ===\n",
            "\n",
            "i highly recommend kindle voyage kindle reader advanced feature amazon kindle voyage extremely enjoy like kindle voyage reader whole bunch like reading whole bunch especially living bible'm christian love god jesus christ nazareth believe product excellent reader person read book kindle voyage amazon library definitely get one today especially light weight awesome back light reading dark like light easy arm holding reading god bless another improvement kindle line moving first generation paperwhite appears right move love new size feel tiny perfect hand new automatic brightness setting work quite well wish could crank force required\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary generation through Flan-T5-large LLM"
      ],
      "metadata": {
        "id": "nmtDJgw9kLAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Setup model and tokenizer\n",
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# 2. Few-shot examples for style guidance\n",
        "few_shot_examples = [\n",
        "    {\n",
        "    \"role\":\"You are an top reviewer for products. You write in complete sentences and high level english like the bloggers from WireCutter and.\",\n",
        "     \"input\": \"Review:\\nGreat vacuum: strong suction, quiet operation, but battery life is short.\\n\\nBlog-style review:\",\n",
        "     \"output\": \"This vacuum excels with its powerful suction and whisper-quiet performance. However, its battery life is on the shorter side, so if you need extended runtime, keep a charger nearby.\"\n",
        "    },\n",
        "    {\n",
        "    \"role\":\"You are an top reviewer for products. You write in complete sentences and high level english like the bloggers from WireCutter and.\",\n",
        "     \"input\": \"Review:\\nSmart speaker with rich audio, sometimes mishears commands. Good value.\\n\\nBlog-style review:\",\n",
        "      \"output\": \"Delivering impressive sound quality at an affordable price, this smart speaker is a solid choice—although occasional voice recognition hiccups can be a minor annoyance.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# 3. Helper to build few-shot prompt\n",
        "def build_prompt(examples, new_review):\n",
        "    prompt_str = \"\"\n",
        "    for ex in examples:\n",
        "        prompt_str += f\"{ex['role']}\\n{ex['input']}\\n{ex['output']}\\n\\n\"\n",
        "    prompt_str += f\"Review:\\n{new_review}\\n\\nBlog-style review:\"\n",
        "    return prompt_str\n",
        "\n",
        "# 4. Load your dataset and combine reviews\n",
        "df = pd.read_csv(\"/content/top3_products.csv\")\n",
        "df[\"prompt\"] = df[\"positive_reviews\"].fillna(\"\") + \"\\n\" + df[\"negative_reviews\"].fillna(\"\")\n",
        "\n",
        "# 5. Run summarization per product\n",
        "for product_name, group in df.groupby(\"name\"):\n",
        "    combined_review = \" \".join(group[\"prompt\"].drop_duplicates().astype(str).tolist())\n",
        "    prompt_text = build_prompt(few_shot_examples, combined_review)\n",
        "\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=600,\n",
        "        num_beams=4,\n",
        "        length_penalty=1.2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\n=== {product_name} ===\\n{summary}\\n\")\n"
      ],
      "metadata": {
        "id": "j_VcyZoyqbG2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "af023718-e69b-407c-f942-284bea62c060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== All-New Fire HD 8 Kids Edition Tablet, 8 HD Display, 32 GB, Blue Kid-Proof Case ===\n",
            "Blog-style review: must-have kid perfect tablet kid parental control allow age appropriate content customizable without spending hour digging setting\n",
            "\n",
            "\n",
            "=== All-New Fire HD 8 Kids Edition Tablet, 8 HD Display, 32 GB, Pink Kid-Proof Case ===\n",
            "This is a great tablet for kids. It's a great alternative to the ipad. It's a great alternative to the ipad. It's a great alternative to the ipad. It's a great alternative to the ipad.\n",
            "\n",
            "\n",
            "=== All-New Fire HD 8 Tablet with Alexa, 8 HD Display, 16 GB, Marine Blue - with Special Offers ===\n",
            "Blog-style review: a+ using couple 'cheaper generic android tablet priced 50-100 say fire tablet beat without question aspect .. faster better display etc etc ... prime member course geared towards prime/etc however easy enuf side load play store get apps need generic tab getting tosed 've already purchased 2nd fire tab amazing little tablet purchased amazon fire hd 8 8 '' tablet 16gb 7th generation 2017 release punch red little 2 month ago use traveling home project state found use plane watch movie use check email surf web listen music play game everything phone except make call easy use picture amazing cost considerably less galaxy 7 '' table cost extra added mine came 16gb storage quickly expanded 64 added cover protect far love tablet glad decided galaxy 7 '' tablet amazing tablet great price got black friday sale less 50 everything play game internet surf create document read book take pictures/videos stream music movie hd even talk alexa feature expandable 256gb memory fast reliable tablet price find equal n't even scratched surface everything reason n't give 5 star camera 2mp resolution surprising considering bothered put hd screen tablet awesome tablet thing great display incredibly sharp wife gone thru 5 tablet 3 cellphone last 2years love fire hd 8. battery last day 's alot easily navigated user-friendly would definitely recommend family friend even complete stranger complaint .... screen share mirroring need included feature otherwise awesome tablet excellent tablet overall since bought tablet able find major fault 8 '' display allows quite portable allowing screen large enough watch movie device micro sd port side allows extend internal memory one slight disadvantage run fire o 5 although variant android limited apps available download installation amazon 's appstore google play store app present tablet fantastic value little tablet purchased moment weakness really wanted kindle reader exceeded need fraction price read listen book plus ton thing including browse wifi  offer '' show lock screen notification ignored dismissed easily make device price 50 promo reasonable speed decent storage upgradable microsd feel like book folio-style case perfect note higher-end productivity tablet device also replace entertainment internet browsing leisure purchased 'm pleased fit need perfectly wanted device browse content screen bigger smart phone read e book watch video cheaper coming renowned brand latest 2017 fire hd 8 improved screen memory battery compared predecessor also found way install google play store use normal android tablet 's link http //www.howtogeek.com/232726/how-to-install-the-google-play-store-on-your-amazon-fire-tablet/.the device run smoothly far installing fav apps good enough tablet excellent price purchased one day sale 50 mainly extra tablet laying around quick access web found useful work amazon shopping apps well alexa/ screen colorful sharp play movie\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-1837571821>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2614\u001b[0m             )\n\u001b[1;32m   2615\u001b[0m             \u001b[0;31m# 12. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2616\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2617\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2618\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   4028\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4030\u001b[0;31m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4032\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1812\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 )\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     ):\n\u001b[0;32m--> 679\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    680\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    595\u001b[0m     ):\n\u001b[1;32m    596\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    598\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;31m# save all key/value_states to cache to be re-used for fast auto-regressive generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mcache_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_position\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cross_attention\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 key_states, value_states = curr_past_key_value.update(\n\u001b[0m\u001b[1;32m    517\u001b[0m                     \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"cache_position\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/cache_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary Generate from Flan-T5-base"
      ],
      "metadata": {
        "id": "6_p3Sdz0Jq6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Setup model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# 2. Few-shot examples for style guidance\n",
        "\n",
        "few_shot_examples = [\n",
        "    {\"role\": \"You are a professional product reviewer writing a concise blog post to help customers choose the best product. Use full sentences and correct english. Do not repeat yourself. Write in the third person about the product\",\n",
        "     \"example\": \"There are no bad iPads. That’s the best news about Apple’s tablet lineup: 15 years after Steve Jobs first debuted the device, the iPad is the best tablet on the market, and it’s not particularly close. Apple’s App Store is enormous and filled with great apps, Apple’s performance and battery life are consistently excellent, and the iPad is still the company’s most versatile device. That’s one easy answer to your question: yes, if you want a tablet you should buy an iPad. Even last year’s iPad, or heck, last-last year’s iPad is still a solid device. Buying an older but better device — last year’s Pro instead of this year’s Air, for instance — is a tried and true iPad formula. The simplest way to pick an iPad is by process of elimination. First, there’s your budget: you can spend $350 on an iPad, you can spend $2,728 on an iPad, or you can spend just about anything in between. You should also decide whether you need an Apple Pencil and which one has the features you need, because not every iPad supports every model. The same goes for the keyboard attachments. Between price and accessories, your choice might be instantly obvious.\"\n",
        "    },\n",
        "    {\"role\": \"You are a professional product reviewer writing a concise blog post to help customers choose the best product. Use full sentences and correct english. Do not repeat yourself. Write in the third person about the product\",\n",
        "    \"example\": \"Today’s robot vacuums are becoming a bit like cars: with all the features, upgrades, and fancy trimmings available these days, it’s easy to forget that they can just be simple machines that get us from point A to point B. Yes, some bots blow hot air on their bums (mop pads), deftly navigate dog poop, and have arms to pick up your socks, but there are plenty of basic budget robot vacuums that just do a decent job of cleaning your floor autonomously — as long as you tidy up first. Fancier models have obstacle recognition, and some even use AI-powered cameras to tell popcorn from poop and avoid the latter. If you want one of those, check out my best robot vacuum buying guide. But if you think you can manage the task of picking up after yourself (and your puppy), a budget bot will save you a lot of money and still do a good job cleaning your floor.\"\n",
        "    },\n",
        "    {\"role\": \"You are a professional product reviewer writing a concise blog post to help customers choose the best product. Use full sentences and correct english. Do not repeat yourself. Write in the third person about the product\",\n",
        "      \"example\": \"So, you’re thinking of buying a smart ring. Well, some good news. Picking the best of the lot is incredibly easy right now. The “bad” news is that, as far as trustworthiness and reliability, your choices are somewhat limited, as this is still a niche and emerging gadget category. Smart rings are in the middle of a resurgence. That means a lot of experimental ideas and newcomer tech brands you’ve probably never heard of. Enough competitors have cropped up that I spent the better part of last summer rocking six rings like a high-tech mafia don. While these aren’t necessarily bad products (some are pretty good), many aren’t as polished as what you’d see in more mature categories like smartwatches, headphones, and smartphones. Speaking of which, there are a few things to know about the category. Currently, these devices are primarily health trackers. Their benefit is they’re more discreet and are better suited to sleep tracking than a smartwatch. However, the vast majority don’t include smart alarms or push notifications. This makes them best suited to casual athletes or more wellness-minded people. Hardcore athletes would be better served in most cases by a smartwatch or fitness tracker, with a smart ring as a supplementary source of data. (But that’s quite an expensive endeavor.) Smart rings are also ill-suited for weightlifters, as they can easily scratch against equipment. With that in mind, here’s the best smart ring for most people in 2025 — and a handful of runners-up worth highlighting for the more tech-adventurous.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# 3. Helper to build few-shot prompt\n",
        "def build_prompt(examples, new_review):\n",
        "    prompt_str = \"\"\n",
        "    for ex in examples:\n",
        "        prompt_str += f\"{ex['role']}\\n{ex['example']}\\n\\n\"\n",
        "    prompt_str += f\"Review:\\n{new_review}\\n\\nBlog-style review:\"\n",
        "    return prompt_str\n",
        "\n",
        "# 4. Load your dataset and combine reviews\n",
        "df = pd.read_csv(\"/content/top3_products.csv\")\n",
        "print(df.head)\n",
        "df[\"prompt\"] = df[\"positive_reviews\"].fillna(\"\") + \"\\n\" + df[\"negative_reviews\"].fillna(\"\")\n",
        "\n",
        "# 5. Run summarization per product\n",
        "for product_name, group in df.groupby(\"name\"):\n",
        "    combined_review = \" \".join(group[\"prompt\"].drop_duplicates().astype(str).tolist())\n",
        "    prompt_text = build_prompt(few_shot_examples, combined_review)\n",
        "\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1000,\n",
        "        num_beams=4,\n",
        "        length_penalty=1.2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\n=== {product_name} ===\\n{summary}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCuD8ZFBuevl",
        "outputId": "0d4c90a7-363a-41c8-d611-4e106d9b656c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of                     asins                                               name  \\\n",
            "0              B00OQVZDJM  Amazon Kindle Paperwhite - eBook reader - 4 GB...   \n",
            "1              B00IOY8XWQ  Kindle Voyage E-reader, 6 High-Resolution Disp...   \n",
            "2              B00IOYAM4I  Kindle Voyage E-reader, 6 High-Resolution Disp...   \n",
            "3   B018SZT3BK,B01AHB9CN2  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "4              B01J94YIT6  All-New Fire HD 8 Tablet with Alexa, 8 HD Disp...   \n",
            "5              B018T075DC  Fire HD 8 Tablet with Alexa, 8 HD Display, 16 ...   \n",
            "6              B01J94SCAM  All-New Fire HD 8 Kids Edition Tablet, 8 HD Di...   \n",
            "7              B01J94SBEY  All-New Fire HD 8 Kids Edition Tablet, 8 HD Di...   \n",
            "8              B018Y226XO  Fire Kids Edition Tablet, 7 Display, Wi-Fi, 16...   \n",
            "9   B00L9EPT8O,B01E6AO69U                   Echo (White),,,\\nEcho (White),,,   \n",
            "10             B06XB29FPF        Amazon - Echo Plus w/ Built-In Hub - Silver   \n",
            "11             B010CEHQTG  Amazon Echo Show Alexa-enabled Bluetooth Speak...   \n",
            "\n",
            "                      cluster_name  rating_mean  rating_count  \\\n",
            "0        E-Reader & Office Tablets         4.77          3175   \n",
            "1        E-Reader & Office Tablets         4.73           599   \n",
            "2        E-Reader & Office Tablets         4.86            51   \n",
            "3            Entertainment Tablets         4.58          2368   \n",
            "4            Entertainment Tablets         4.58           831   \n",
            "5            Entertainment Tablets         4.60           455   \n",
            "6   Health & household accessories         4.65           191   \n",
            "7   Health & household accessories         4.63           233   \n",
            "8   Health & household accessories         4.53          1663   \n",
            "9      Smart Home & Amazon devices         4.67          5944   \n",
            "10     Smart Home & Amazon devices         4.75           590   \n",
            "11     Smart Home & Amazon devices         4.66           640   \n",
            "\n",
            "                                            imageURLs  weighted_score  \\\n",
            "0                                                 NaN        4.732904   \n",
            "1   https://i5.walmartimages.com/asr/5bb13520-39a6...        4.624555   \n",
            "2   http://pisces.bbystatic.com/image2/BestBuy_US/...        4.532403   \n",
            "3   https://www.barcodable.com/images/barcode/0841...        4.565829   \n",
            "4   https://www.barcodable.com/images/barcode/0841...        4.549514   \n",
            "5   http://pisces.bbystatic.com/image2/BestBuy_US/...        4.547051   \n",
            "6   https://www.barcodable.com/images/barcode/0841...        4.540680   \n",
            "7   https://www.barcodable.com/images/barcode/0841...        4.540587   \n",
            "8   http://pisces.bbystatic.com/image2/BestBuy_US/...        4.522849   \n",
            "9                                                 NaN        4.656652   \n",
            "10  https://c1.neweggimages.com/NeweggImage/Produc...        4.634473   \n",
            "11  https://i5.walmartimages.com/asr/db02a974-11af...        4.589176   \n",
            "\n",
            "                                     positive_reviews  \\\n",
            "0   daughter 's 1st kindle several kindle version ...   \n",
            "1   amazing portable lightweight third amazon kind...   \n",
            "2   expect improvement pricey yes ... serious read...   \n",
            "3   7 year old granddaughter love request 7 yr old...   \n",
            "4   a+ using couple 'cheaper generic android table...   \n",
            "5   exceptional tablet amazon fire hd 8 awesome de...   \n",
            "6   easy use bought 2 year old since kept fighting...   \n",
            "7   must-have kid perfect tablet kid parental cont...   \n",
            "8   great little computer keep child entertained a...   \n",
            "9   love echo want device performs command alexa n...   \n",
            "10  alexa rock present husband originally asked ra...   \n",
            "11  practical easy setup well designed good sound ...   \n",
            "\n",
            "                                     negative_reviews  \n",
            "0   transfer library book via usb purchased ereade...  \n",
            "1   amazon voyage dead trying update software kind...  \n",
            "2                                                 NaN  \n",
            "3   color difference huge bad customer service got...  \n",
            "4   stopped charging 6 month amazon fire hd 8 tabl...  \n",
            "5   watching video great .. .. hulu play perfectly...  \n",
            "6                                                 NaN  \n",
            "7   amazon fire hd 8 kid edition ok look screen am...  \n",
            "8   aware require credit card purchased 2 christma...  \n",
            "9   work properly device stay connected wifi alway...  \n",
            "10                                                NaN  \n",
            "11  pas waited couple month review giving amazon t...  >\n",
            "\n",
            "=== All-New Fire HD 8 Kids Edition Tablet, 8 HD Display, 32 GB, Blue Kid-Proof Case ===\n",
            "Review: must-have kid perfect tablet kid parental control allow age appropriate content customizable without spending hour digging setting come 2-year question asked adh warranty amazon making geek squad protection device irrelevant also come free 1-year subscription amazon kid\n",
            "\n",
            "\n",
            "=== All-New Fire HD 8 Kids Edition Tablet, 8 HD Display, 32 GB, Pink Kid-Proof Case ===\n",
            "Review: easy use bought 2 year old since kept fighting sister ipads great alternative cost much less 12 month freetime great prime member get big discount continue service 12 free month build quality great free child bumper work great\n",
            "\n",
            "\n",
            "=== All-New Fire HD 8 Tablet with Alexa, 8 HD Display, 16 GB, Marine Blue - with Special Offers ===\n",
            "review: a+ using couple 'cheaper generic android tablet priced 50-100 say fire tablet beat without question aspect .. faster better display etc etc ... prime member course geared towards prime/\n",
            "\n",
            "\n",
            "=== All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Black ===\n",
            "Review: 7 year old granddaughter love request 7 yr old granddaugter always asked older sister could share due bought tablet/size yr earlier loved security/parental control options.starfall favorite\n",
            "\n",
            "\n",
            "=== Amazon - Echo Plus w/ Built-In Hub - Silver ===\n",
            "Review: alexa rock present husband originally asked radio home office set relatively quick amazed sound quality loved versatility could specific radio station played random song well tell joke etc fun u looking getting smaller  dot '\n",
            "\n",
            "\n",
            "=== Amazon Echo Show Alexa-enabled Bluetooth Speaker with 7\" Screen ===\n",
            "Review: practical easy setup well designed good sound everything alexa plus hd video always ready answer associated video text applicable show movie trailer also watch amazon video excellent on-demand security video amazon compatible camera voice activated\n",
            "\n",
            "\n",
            "=== Amazon Kindle Paperwhite - eBook reader - 4 GB - 6 monochrome Paperwhite - touchscreen - Wi-Fi - black,,, ===\n",
            "Review: daughter 's 1st kindle several kindle version daughter bought birthday love reading several series n't carry bag full bulky book trip use light read night store many book small space want\n",
            "\n",
            "\n",
            "=== Echo (White),,,\n",
            "Echo (White),,, ===\n",
            "Review: love echo want device performs command alexa name.she 's constant companion kitchen sings tell joke share recipe tell story quote bible verse give weather forecast much more.if add additional device control\n",
            "\n",
            "\n",
            "=== Fire HD 8 Tablet with Alexa, 8 HD Display, 16 GB, Tangerine - with Special Offers ===\n",
            "Review: exceptional tablet amazon fire hd 8 awesome device price tablet mind boggling tablet everything would expect want tablet amazon store amazon prime preloaded 's much fun easy use performance tablet unbelievable never stop thinking modest price\n",
            "\n",
            "\n",
            "=== Fire Kids Edition Tablet, 7 Display, Wi-Fi, 16 GB, Pink Kid-Proof Case ===\n",
            "Great little computer keep child entertained amazon fire kid edition tablet thousand child-friendly e-books tv show game educational apps accessible tablet engage child healthy way kid-proof case prevents easy damage expandable\n",
            "\n",
            "\n",
            "=== Kindle Voyage E-reader, 6 High-Resolution Display (300 ppi) with Adaptive Built-in Light, PagePress Sensors, Free 3G + Wi-Fi - Includes Special Offers ===\n",
            "Review: expect improvement pricey yes ... serious reader user library e-books completly worth nothing beat real book experience traveling 4 day plus per week nothing beat portability ability add library amazon fly ... never seek\n",
            "\n",
            "\n",
            "=== Kindle Voyage E-reader, 6 High-Resolution Display (300 ppi) with Adaptive Built-in Light, PagePress Sensors, Wi-Fi - Includes Special Offers ===\n",
            "Review: amazing portable lightweight third amazon kindle far best one adaptive lighting really help easy eye battery last week normal charge charge full pretty fast.i still enjoy eink display regular phone tablet extended reading periods.the kindle\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    EncoderDecoderModel,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "# 1. Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "# 2. Build Encoder-Decoder\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    \"xlm-roberta-base\",  # encoder\n",
        "    \"facebook/bart-base\"  # decoder (autoregressive, supports generation)\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(\"/content/top3_products.csv\")\n",
        "dataset[\"prompt\"] = dataset[\"positive_reviews\"].fillna(\"\") + \"\\n\" + dataset[\"negative_reviews\"].fillna(\"\")\n",
        "\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"source_text\"], max_length=1024, truncation=True)\n",
        "    labels = tokenizer(examples[\"target_summary\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "# set-up training arguements\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "\n",
        "#initialize the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Tokenize & summarize\n",
        "sequence = (\"Long product reviews ...\")\n",
        "inputs = tokenizer(sequence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "# 4. Generate\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_new_tokens=150,\n",
        "    num_beams=4,\n",
        "    length_penalty=1.2,\n",
        ")\n",
        "\n",
        "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "aqp2kFwBVi5r",
        "outputId": "83995b19-7299-4251-8353-017770275f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-3383281386>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 1. Initialize tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlm-roberta-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 2. Build Encoder-Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2023\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2025\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2026\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2027\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2277\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2278\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2279\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mimport_protobuf_decode_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2280\u001b[0m             logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mmask_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstrip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrstrip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfast_tokenizer_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_slow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# We have a serialization from tokenizers which let us directly build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_tokenizer_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mslow_tokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# We need to convert a slow tokenizer to build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}